<!-- START doctoc generated TOC please keep comment here to allow auto update -->
<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->
**Table of Contents**  *generated with [DocToc](https://github.com/thlorenz/doctoc)*

- [第三章：在线程间共享数据](#%E7%AC%AC%E4%B8%89%E7%AB%A0%E5%9C%A8%E7%BA%BF%E7%A8%8B%E9%97%B4%E5%85%B1%E4%BA%AB%E6%95%B0%E6%8D%AE)
  - [线程间共享数据的问题](#%E7%BA%BF%E7%A8%8B%E9%97%B4%E5%85%B1%E4%BA%AB%E6%95%B0%E6%8D%AE%E7%9A%84%E9%97%AE%E9%A2%98)
  - [用互斥保护共享数据](#%E7%94%A8%E4%BA%92%E6%96%A5%E4%BF%9D%E6%8A%A4%E5%85%B1%E4%BA%AB%E6%95%B0%E6%8D%AE)
  - [保护共享数据的其他方式](#%E4%BF%9D%E6%8A%A4%E5%85%B1%E4%BA%AB%E6%95%B0%E6%8D%AE%E7%9A%84%E5%85%B6%E4%BB%96%E6%96%B9%E5%BC%8F)

<!-- END doctoc generated TOC please keep comment here to allow auto update -->

# 第三章：在线程间共享数据

## 线程间共享数据的问题

- 不变量被广泛使用来表示程序对特定数据的断言，该断言总是成立的，如果不成立则说明程序出现了错误。比如说双向链表中每个节点的下一个节点的上一个节点一定指向自己。
- 数据结构在设计中每个修改操作前后不变量都成立，但是中途修改过程中不一定成立。
- 单线程程序中，这不会有问题，但多线程中，可能会在不变量不成立时被修改造成其他线程读取了一个非法状态，这就是数据竞争。
- 为了防止数据竞争，通常的方法有：
    - 采取包装错误包装数据结构，在不变量破坏时，中间状态只对执行修改的线程可见。其他线程视角中要么修改还未开始，要么已经完成，其他线程不可能访问到不变量破坏的状态。经典做法是使用互斥锁保护对不变量的修改代码。
    - 修改数据结构设计及其不变量，由一连串不可拆分的改动完成操作，并且每个改动都维持不变量。这种方式称为无锁编程。
    - 还有一种方式是将要进行的写操作视为一个操作序列称为事务，执行修改时提交事务，每个事务完成后才能执行下一个事务。数据库中常用，但实际编程不是很多，这里不讨论。

## 用互斥保护共享数据

在不变量被破坏的区域、以及访问这段区域中会进行修改的数据的区域，称为临界区，进入前使用互斥锁定，其他线程要进入就只有等待，就保护了不变量不被破坏。
- 如果利用互斥锁设计程序接口，需要谨慎设计接口，避免将任何内部数据的引用或者指针暴露出来，所有修改和访问操作都需要通过成员函数完成，并在其中使用互斥保护。保证对数据的任何访问都受到了保护，不能留后门。
- 无论什么形式，参数、返回值、指针、引用，都不能将内部数据的访问和修改权直接暴露出来。同时需要注意在受保护的函数内部有没有调用其他函数，然后将指针引用传递后被保存了起来，等到脱离了临界区后再访问，这也是一种暴露。
- 总而言之，必须做到一点，所有对数据的访问都需要用互斥保护。

接口固有的条件竞争：
- 就算所有接口都使用互斥进行了妥善保护，也有可能出现条件竞争。
- 典型情况是，上一个接口返回值不可信，在执行完后数据结构可能被更改使得返回值不成立，基于这个返回值作为条件的操作可能是非法的。比如一个栈`stack`，先`empty`判空再`top`取栈顶元素，判空和取元素都得到保护，但是判空后可能其他线程弹出了元素，使得条件发生改变。
- 这个问题的根源在于一个操作被切分了，解决方法只能是变更接口。
- 就上面的例子可将`top`设计为空时抛异常，不为空则返回栈顶元素。虽然看起来解决了问题，但是用起来却很麻烦，因为必须捕获异常。
- 普遍做法是将外部可能进行的操作设计为一个操作，用互斥保护，并对错误情况进行合理的处理和报告。但是这种做法很可能异常不安全，如果其中一个操作抛异常，可能出现另一个操作已经执行却无法恢复的情况。异常安全要求我们分割操作一个接口做一件事情，但是线程安全要求我们将操作合并到一个接口中，这种矛盾是天然的，无法很好解决。
- 就上面的问题，我们需要将`top pop`合并为一个操作，解决方案有：
    - 传入一个容器，或者值引用，将元素写入参数中。这要求元素能够赋值或者交换。
    - 按值返回元素也是一个解决方案，并且要求元素能够移动，且移动操作不能抛出异常。这样即保证了线程安全也保证了异常安全。
    - 返回指针，指向弹出元素。为了避免内存管理问题，可以返回智能指针。
    - 实践中可以结合多种方法，提供多个接口，每种情况下至少都留一个选择，就能最大程度适应所有情况。
- 一个线程安全的栈的实现见[P49.ThreadSafeStack.cpp](P49.ThreadSafeStack.cpp)。

锁的粒度：
- 锁的粒度需要谨慎设计，粒度太小，互斥可能出现问题，起不到保护作用。
- 粒度太大，可能消解并发的优势，在共享大量数据的系统中同时只有一个线程工作，其他线程都在等待。
- 为了更精细地控制加锁粒度，可能需要多个锁进行更精细的控制，某些时候就需要获取多个锁。这很容易导致死锁，应该通过`std::scoped_lock`这种带有死锁解除算法的RAII类来同时加多个锁。

死锁：
- 对于需要获取多个锁的情况，为了防范死锁，通常建议是多个线程中按照相同顺序加锁。但是这比较难以保证，非常容易出错。比如多个线程中调用同一函数，接收多个参数，并对每一个参数加锁，但是多个线程中参数顺序不确定，那么就无法保证按照相同顺序加锁。
- 所以标准做法是使用`std::lock`，其实现了死锁避免算法，要么同时加锁，如果无法做到就阻塞并释放所有锁，只有同时获取到了所有锁才会返回。同时利用RAII类管理的话可以这样写：
```C++
std::lock(m1, m2);
std::lock_guard lock1(m1, std::adopt_lock);
std::lock_guard lock2(m2, std::adopt_lock);
```
- C++17引入了`std::scoped_lock`作为`std::lock`的包装，可以直接用：
```C++
std::scoped_lock lock(m1, m2);
```
- `std::lock std::scoped_lock`可以在同时获取多个锁时避免死锁，但如果需要分别获取多个锁，那么他们就用不了了。
- 虽然死锁是多线程程序最棘手的问题之一，纵然绝大多数情况一切正常，但死锁往往无法预测。尽管如此，只要编写的代码服从一些相对简单的规则，便有助于防范死锁。

**死锁防范的补充准则**：
- 死锁并不一定只在使用锁时才会出现，比如两个线程互相在对方的`std::thread`实例上调用`join`也会出现死锁。
- 死锁防范的准则最终可归纳为一个思想：只要另一线程正在等待当前线程，那么当前线程就不能反过来等它。下面的准则列举了各种方法，用于判断和排除其他线程是否在等待当前线程。
- 1、避免嵌套锁：
    - 假如已经持有锁，就不要试图去获取第二个锁。
    - 如果能够恪守这一点，就不可能在锁的使用上导致死锁。
    - 如果确实需要获取多个锁，则使用`std::lock std::scoped_lock`。
- 2、一旦持锁，则避免调用用户定义的接口：
    - 我们无法知道用户自定义实现会做什么操作，它可能随意操作，包括试图获取锁（此时便违反了第一条）。
    - 如果持锁之后的逻辑全部由我们控制，那么能够得到最大的保证。
- 3、依从固定顺序获取锁：
    - 如果多个锁是绝对必要的，但却无法同时一步使用`std::lock std::scoped_lock`获取。那么只能退而求其次，保证在每个线程内部都依从固定顺序获取锁。
    - 关键是事先规定加锁顺序，要求所有线程遵守。
    - 举个例子：可以对线程安全的双向链表再细分加锁粒度，每个节点都可以加锁：那么删除节点时就需要对删除节点以及前后两个节点加锁；遍历链表时可以先对当前节点加锁，然后获取下一个节点的锁，获取成功则释放当前节点，这样既可以多个线程同时访问链表，也不会造成死锁，但是应该只能从一个方向遍历，如果允许两个线程正反两个方向同时遍历，他们相遇时就会出现死锁。
    - 这个例子中解决方法很简单：规定遍历方向，并且加锁时全部从头加到尾（比如删除需要加三个锁，那么先加上一个节点，再加要删除节点，最后加下一个节点），不能反方向。
    - 并发数据结构也会受到这条规则限制和串行数据结构有一些不同。
- 4、按层级加锁：
    - 锁的层级划分就是按照特定方式规定加锁次序，在运行时检查加锁操作是否遵从预设规则。
    - 按照构思，将应用程序分层，并且明确每个互斥锁位于哪个层级，若线程已经对低层级互斥加锁，则不准再对高层级互斥加锁。
    - 具体做法是将层级编号赋予对应层级应用程序上的互斥，并记录各线程分别锁定了哪些互斥。
    - 这种模式C++标准库并未直接支持，需要自行编写定制的`hierarchical_mutex`。
    - 如果互斥加锁违反规则，从锁的实现上就规定了，可以抛出异常或者终止程序，以防范死锁。
    - 一个简单的`hierarchical_mutex`实现见[P57.HierarchicalMutex.cpp](P57.HierarchicalMutex.cpp)。
- 5、将准则推广到锁操作之外：
    - 死锁不仅仅因加锁操作而发生，任何同步机制的循环等待等可能导致死锁出现。在所有情况中都可以推广上述准则。
    - 应尽可能避免嵌套锁（递归锁）。
    - 若当前线程持有某个锁，应避免等待其他线程（条件变量、`join`、锁等等）。
    - 除了使用`std::lock_guard std::scoped_lock`这种RAII类，还可以使用`std::unqiue_lock`更灵活地管理互斥锁。

使用`std::unique_lock`灵活加锁：
- `std::unique_lock`放宽了不变量的成立条件，比`std::lock_guard`更灵活，可以对其中保存的互斥锁单独进行加锁、释放操作。
- 可以用其来管理已经锁定的互斥锁、或者管理时不去锁定后续再调用`lock`接口去锁定。
- `std::unique_lock`中存储了并且会更新互斥锁的是否锁定的标志，析构函数会根据这个信息决定是否需要`unlock`，可以使用`owns_lock`查询。
- 构造时传入`std::defer_lock std::try_to_lock std::adopt_lock`可以延迟锁定、尝试锁定、获取已锁定的锁，非常灵活。
- 相比`std::lock_guard`对象大小和开销都会更大一些。如果`std::lock_guard`能够满足要求，应该优先使用。
- `std::unique_lock`不可赋值只可移动，可用于在不同作用域之间转移锁归属权。
- `std::unique_lock`可以作为通道类，以移动方式返回数据结构内部锁所有权，接着执行单纯的数据结构成员无法完成的操作，在执行完操作后再释放。也可以起到保护共享数据的作用，还能一定程度缓解必须将数据结构操作合并起来实现的问题，算是不同的编程风格。
- 比如前面的例子：
```C++
if (!q.empty())
{
    value = q.top();
    q.pop();
}
```
- 前面说过这样实现是线程不安全的，使用这种风格实现后可以改成这样：
```C++
{
    std::unique_lock<std::recursive_mutex> lck;
    if (!q.empty(lck))
    {
        value = q.top();
        q.pop();
    }
}
```
- 这样实现必须要能够重复对同一互斥加锁，所以需要使用`std::recursive_lock`，至于这样实现是好还是坏，要看具体场景。例子见[P61.TransferOwnershipOfLock.cpp](P61.TransferOwnershipOfLock.cpp)。
- 这样实现可以避免使用异常来报告错误，不失为一种合理的选择。

按合适粒度加锁：
- 两个要点：
    - 选择足够大粒度的锁，确保目标数据受到保护。
    - 限制范围，只在必要的操作过程中持有锁。
- 特别注意：持有锁期间应避免任何耗时的操作，比如读写文件，尽量快速做完操作后释放锁。除非锁是为了保护文件访问，否则毫无必要阻塞其他线程。
- 如果某些中间步骤不需要加锁，并且前后需要加锁的操作不会破坏不变量，那么可以在中间解锁，在后续需要访问共享数据时又加锁，通过`std::unique_lock`可以轻松实现。
- 对于取多个对象的某个小数据进行操作的情况，尽量单独加锁取其拷贝，而非尝试获取获取所有锁，这能有效提升效率。但做这种改动要谨慎，避免这种改动影响语义，比如说这时两个对象中取值的时刻就不一样了。
- 某些时候数据结构的访问也需要采取不同层级的访问，固定粒度可能并不合适。

## 保护共享数据的其他方式

**在初始化过程中保护共享数据**：
- 互斥是保护共享数据最普遍的方式，但并不是唯一方式，针对具体场景还有别的方式提供更合适的保护措施。
- 比如只需要在初始化中保护的数据，初始化完成后所有访问都是读取，不需要保护。
- 某些时候我们需要某个共享数据，并且它初始化开销不菲，所以通常用到时才初始化。但是初始化过程可能会造成数据竞争，初始化完成后则不会，所以通常会这样实现：
```C++
std::shared_ptr<some_resource> resource_ptr;
std::mutex resource_mutex;
void foo()
{
    std::unique_ptr<std::mutex> lck(resouce_mutex);
    if (!resource_ptr)
    {
        resource_ptr.reset(new some_resource);
    }
    lck.unlock();
    resource_ptr->do_something();
}
```
- 仅保护初始化时，但是也需要用到互斥锁，并且后续访问时都需要获取互斥锁并释放以确保资源已经初始化。这造成了不必要的开销。
- 很多人尝试对这种方式进行改进，并且衍生除了一种饱受诟病的双重检验锁定模式（double-checked locking pattern）：首先无锁读取，读取到空指针才加锁，然后再进行一次判断，为空才初始化。
```C++
std::shared_ptr<some_resource> resource_ptr;
std::mutex resource_mutex;
void foo()
{
    if (!resource_ptr)
    {
        std::unique_ptr<std::mutex> lck(resouce_mutex);
        if (!resource_ptr)
        {
            resource_ptr.reset(new some_resource);
        }
        lck.unlock();
    }
    resource_ptr->do_something();
}
```
- 这种模式可能出现当前线程执行`do_something`对资源已经被其他线程初始化并修改过的事情不知情，仍将其当做未初始化的资源的情况。在C++中不太能行得通（其实条件很苛刻，而且后续都是只读操作的话其实是不影响的）。
- C++11标准引入了`std::once_flag std::call_once`专门解决这种情况。保证多个同时调用`std::call_once`函数的线程最终初始化由其中某一个线程安全唯一地完成。必要的同步数据由`std::once_flag`存储。
```C++
std::shared_ptr<some_resource> resource_ptr;
std::once_flag resource_flag;
void init_resource()
{
    resource_ptr.reset(new some_resource);
}
void foo()
{
    std::call_once(resource_flag, init_resource);
    resource_ptr->do_something();
}
```
- 即是是某个类的数据成员、静态数据成员，也能很好的初始化。只要注意`std::once_flag`和资源本身是同样的生命周期即可。
- C++11标准前，传统的局部静态数据第一次运行时初始化，在多线程环境中初始化时时可能出现数据竞争的。C++11标准解决了这个问题，规定初始化只会发生在某一线程上，初始化完成前其他线程不会越过静态数据声明而继续运行。 
- 如果是所有线程使用的全局实例，那么可以用局部静态变量代替`std::call_once`：
```C++
void foo()
{
    static std::shared_ptr<some_resource> resource_ptr(new some_resource);
    resource_ptr->do_something();
}
```
- 总体来说这种使用方式还是仅算一种特例，使用并不是很多。更多的情况是保护哪些很少更新，大部分时候都是读取的数据结构。

**保护很少更新的数据结构**：
- 在一些场景中，我们会使用一种更新很少，绝大多数访问都是读取的数据结构。这时如果对所有读取操作加锁，可能没有太大必要，因为很有可能这时候并没有线程在写入，这就造成了性能的浪费。
- 我们可能会想要如果线程在更新操作时，那么才表现出锁的行为，其他线程此时不能读取。但更新完成后，数据结构又可以不需要锁定就可以多线程并发访问。显然`std::mutex`做不到这一点，所以需要一种新互斥锁。
- 这种互斥锁具有两种不同的使用方式，所以也称之为读写互斥。允许单个写线程完全排他访问，也允许多个读线程共享数据或者并发访问。
- C++17标准库提供了两种互斥：`std::shared_mutex std::shared_timed_mutex`提供了这个功能，（其中`std::shared_timed_mutex`C++14就引入了）。
- 相比`std::mutex`而言，`std::shared_mutex`多了三个接口：`lock_shared try_lock_shared unlock_shared`，这三个接口是以共享模式锁定，锁定之后其他线程依然可以以共享模式锁定，`lock try_lock unlock`依然是使用独占模式锁定，锁定之后则不可共享锁定。
- `std::shared_timed_mutex`同样提供了带时限的共享锁定`try_lock_shared_for try_lock_shared_until`。
- 进行独占锁定时依然可以使用`std::lock_guard std::unique_lock std::scoped_lock`这些RAII类，并且提供了专门的RAII类`std::shared_lock`进行共享锁定，其中的`lock try_lock try_lock_for try_lock_until unlock`会调用对应的共享锁定接口。
- 当`std::shared_mutex`有一个线程进行独占锁定时，其他线程的所有锁定都会阻塞。当有一个或者多个线程进行共享锁定时，其他线程依然可以共享锁定，但是独占锁定则会阻塞等到所有共享锁定释放以后才能获取。
- 这两个锁可以用于多个读取线程多个写线程、多个读取线程一个写线程的场景。
- 例子见[P69.SharedMutex.cpp](P69.SharedMutex.cpp)。

递归加锁：
- `std::recursive_lock std::recursive_timed_lock`可以递归锁定，在同一个线程中多次锁定，释放次数必须与锁定次数匹配。
- 通常用于一个加锁的函数会调用另一个加锁的函数情况，这种情况下使用普通互斥锁被重复锁定将不能完成任务。
- 但这种设计是可以修改的，在加锁时通常不变量会被破坏，此时调用的另一个公有函数将工作于这种环境下。通常来说这不是一种好设计，还会带来额外开销。
- 通常的修改是一个加锁的公有函数永远不要调用另一个加锁的公有函数，而是将公共部分提取成私有函数，工作在假定已经加锁的场景下。
- 前面提到过的将锁所有权暴露出来给外部使用时可能依然需要递归锁，因为外部只能调用公有函数，使用递归锁才能有效工作。
- 总而言之：递归锁有好处，但容易被滥用，如果能使用普通互斥锁，就应该用普通互斥锁。
